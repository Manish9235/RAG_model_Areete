{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**OBJECTIVE**\n",
        "- To develop a Retrieval Augmented Generation (RAG) model. The RAG model should be capable of\n",
        "retrieving relevant information from the PDF and generating coherent responses or\n",
        "summaries based on user queries and maintain the chain of conversion till the end."
      ],
      "metadata": {
        "id": "Kth7fhdgDQNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Development**\n",
        "- Develop a Retrieval Augmented Generation model.\n",
        "- Implement a retrieval mechanism to efficiently search for relevant passages from the PDF document based on user queries.\n",
        "- Design the generation component to produce coherent responses or summaries based on the retrieved passages."
      ],
      "metadata": {
        "id": "vAwZdFqvEFMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-To develop a Retrieval Augmented Generation (RAG) model using a dataset stored in PDF format, we need to follow these steps systematically:\n",
        "\n",
        "- Data Extraction from PDFs\n",
        "- Preprocessing and Indexing\n",
        "- Training the Retrieval Component\n",
        "- Training the Generation Component\n",
        "- Integration and Fine-tuning\n",
        "- Inference and Testing"
      ],
      "metadata": {
        "id": "l15WTt0QM2GU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Extraction from PDF\n",
        "First we need to extract text from PDF files"
      ],
      "metadata": {
        "id": "u7f4o_JzO_Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMPPRc5BRSCG",
        "outputId": "3271f26d-218c-43b1-dc68-b3a29de0012c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.4-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.4 PyMuPDFb-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0fy5GV9DBA8",
        "outputId": "23581a26-b98a-44db-b435-62fcc46e1d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handbook of \n",
            "Good Dairy Husbandry Practices\n",
            "National Dairy\n",
            "Development  Board\n",
            "2\n",
            "BREEDS OF INDIGENOUS DAIRY CATTLE\n",
            " \n",
            "BULLS   \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "    COWS\n",
            "Kankrej\n",
            "Native tract: Kutch, Mehsana & \n",
            "Banaskantha districts of Gujarat\n",
            "Tharparkar\n",
            "Native tract: Jaisalmer, Barmer \n",
            "and Jodhpur districts in  \n",
            "Rajasthan\n",
            "Red Sindhi\n",
            "Native tract: In Pakistan, also \n",
            "found in Punjab, Haryana & \n",
            "Rajasthan & Uttarakhand\n",
            "Rathi\n",
            "Native tract: Bikaner & \n",
            "Shri Ganganagar districts of \n",
            "Rajasthan\n",
            "Sahiwal\n",
            "Native tract: Ferozpur and \n",
            "Amritsar district of Punjab & Shri \n",
            "Ganganagar district of Rajasthan\n",
            "Hariana\n",
            "Native tract: Rohtak, Hissar, \n",
            "Sonepat, Gurgaon, Jind and \n",
            "Jhajjar districts in Haryana\n",
            "Gir \n",
            "Native tract: Junagadh, Rajkot, \n",
            "Bhavnagar and Amreli districts \n",
            "in Gujarat\n",
            "BREEDS OF BUFFALO\n",
            " \n",
            "    BULLS \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  COWS\n",
            "Murrah\n",
            "Native tract: Hissar, Rohtak, \n",
            "Gurgaon and Jind districts of \n",
            "Haryana\n",
            "Surti\n",
            "Native tract: Anand, Kheda \n",
            "and Baroda districts of \n",
            "Gujarat\n",
            "Mahesani\n",
            "Native tract: Mahesana, \n",
            "Banaskantha & Sabarkantha \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pymupdf\n",
        "pdf_path = \"/content/Knowledge base for RAG-Handbook-of-Good-Dairy-Husbandry-Practices_.pdf\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Preprocessing and Indexing**\n",
        "- Once we have the text extracted, we need to preprocess it (e.g. tokenization, cleaning) and then index it for efficient retrieval"
      ],
      "metadata": {
        "id": "pXH3mN_FSCTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBmifdNFTFn4",
        "outputId": "ed06a3bd-0d9a-493e-a881-7d9b07ca26fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import faiss\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  return sentences\n",
        "\n",
        "# Example usage\n",
        "sentences = preprocess_text(text)\n",
        "\n",
        "# Create TF-IDF embeddings\n",
        "vectorizer = TfidfVectorizer()\n",
        "embeddings = vectorizer.fit_transform([sentence for sentence in sentences])\n",
        "\n",
        "# Create FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings.toarray())\n",
        "\n",
        "# Save the index and vectorizer\n",
        "faiss.write_index(index, 'index.faiss')\n",
        "import pickle\n",
        "with open('vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p74yztKZRlGQ",
        "outputId": "c2f9a8ae-0624-4693-8c73-703d1e8e9891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3: Training the Retrieval Component**\n",
        "- For retrieval component we will use TF-IDF-based FAISS index"
      ],
      "metadata": {
        "id": "z4EYwci6U53r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Step 4: Training the generation Component**\n",
        "- We can use Hugging Face's Transformers library to fine-tune a generative model like GPT"
      ],
      "metadata": {
        "id": "R4TbQ0oVXRhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "689xCjgQc4MP",
        "outputId": "192f3718-8d34-4a25-c55a-c71f088938fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuJ-AmluxFk9",
        "outputId": "fe34dc35-971f-4be8-f497-e6deec8fe41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizerFast, Trainer, TrainingArguments\n",
        "import torch\n",
        "# Load pretrained BART tokenizer and model\n",
        "Tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# We have a dataset of (question, context, answer) tuples\n",
        "\n",
        "train_data = [\n",
        "    {\"question\":\"Why animal health is important?\", \"context\":\"Animal health plays an important role in ...\", \"answer\": \"A diseased animal cannot perform to the expected level. Timely intervention is therefore pivotal in reducing the economic losses due to diseases.\"}\n",
        "]\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        inputs = self.tokenizer(item[\"question\"], item[\"context\"], truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
        "        outputs = self.tokenizer(item[\"answer\"], truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
        "        inputs['labels'] = outputs.input_ids\n",
        "        return inputs\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = CustomDataset(train_data, Tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./result\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "SS2xoMMuWAKE",
        "outputId": "54755423-4946-469a-c6f6-8d982ad26007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=11.442729949951172, metrics={'train_runtime': 1.9757, 'train_samples_per_second': 1.518, 'train_steps_per_second': 1.518, 'total_flos': 3250656903168.0, 'train_loss': 11.442729949951172, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Integration and Fine-tuning**\n",
        "- Integrate the retrieval and generation components. So during inference, the retrival component fetches relevant passages, which are then used by the generative model to produce a response."
      ],
      "metadata": {
        "id": "bxdg69lXdEzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Extract Text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = pymupdf.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Preprocess Text\n",
        "def preprocess_text(text, chunk_size=100):\n",
        "    sentences = text.split('.')\n",
        "    chunks = [' '.join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Implement Search Mechanism\n",
        "class PDFSearchEngine:\n",
        "    def __init__(self, pdf_path):\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        self.chunks = preprocess_text(text)\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.chunk_vectors = self.vectorizer.fit_transform(self.chunks)\n",
        "\n",
        "    def search(self, query, top_n=3):\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, self.chunk_vectors).flatten()\n",
        "        top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "        results = [(self.chunks[i], similarities[i]) for i in top_indices]\n",
        "        return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "query = \"Why animal health is important?\"\n",
        "response = PDFSearchEngine.search(pdf_path, query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "bf2TTikSdEPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}